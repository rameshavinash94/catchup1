{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CatchUp1_final_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshavinash94/catchup1/blob/main/CatchUp1_final_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA00wBE2Ntdm"
      },
      "source": [
        "### Import TFRS\n",
        "\n",
        "First, install and import TFRS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yzAaM85Z12D"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3oYt3R6Nr9l"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxQ1CZcO2wh"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-mxBYjdO5m7"
      },
      "outputs": [],
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
        "\n",
        "# Select the basic features.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"]\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0HSfmSNCWm"
      },
      "source": [
        "Build vocabularies to convert user ids and movie titles into integer indices for embedding layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I1VTEjHzpfX"
      },
      "outputs": [],
      "source": [
        "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
        "\n",
        "movie_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
        "movie_titles_vocabulary.adapt(movies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrch6rVBOB9Q"
      },
      "source": [
        "### Define a model\n",
        "\n",
        "We can define a TFRS model by inheriting from `tfrs.Model` and implementing the `compute_loss` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5dNbDZwOIHR"
      },
      "outputs": [],
      "source": [
        "class MovieLensModel(tfrs.Model):\n",
        "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
        "  # these are still plain Keras Models.\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      user_model: tf.keras.Model,\n",
        "      movie_model: tf.keras.Model,\n",
        "      task: tfrs.tasks.Retrieval):\n",
        "    super().__init__()\n",
        "\n",
        "    # Set up user and movie representations.\n",
        "    self.user_model = user_model\n",
        "    self.movie_model = movie_model\n",
        "\n",
        "    # Set up a retrieval task.\n",
        "    self.task = task\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # Define how the loss is computed.\n",
        "\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\n",
        "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "\n",
        "    return self.task(user_embeddings, movie_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdwtgUCEOI8y"
      },
      "source": [
        "Define the two models and the retrieval task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvtnUN6aUY4U"
      },
      "outputs": [],
      "source": [
        "# Define user and movie models.\n",
        "user_model = tf.keras.Sequential([\n",
        "    user_ids_vocabulary,\n",
        "    tf.keras.layers.Embedding(user_ids_vocabulary.vocabulary_size(), 64)\n",
        "])\n",
        "movie_model = tf.keras.Sequential([\n",
        "    movie_titles_vocabulary,\n",
        "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocabulary_size(), 64)\n",
        "])\n",
        "\n",
        "# Define your objectives.\n",
        "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
        "    movies.batch(128).map(movie_model)\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMV0HpzmJGWk"
      },
      "source": [
        "\n",
        "### Fit and evaluate it.\n",
        "\n",
        "Create the model, train it, and generate predictions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2tQDhqkOKf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d27b3e8-ce5b-409d-b015-11cc84b0dfb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "25/25 [==============================] - 9s 289ms/step - factorized_top_k/top_1_categorical_accuracy: 1.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0017 - factorized_top_k/top_10_categorical_accuracy: 0.0052 - factorized_top_k/top_50_categorical_accuracy: 0.0444 - factorized_top_k/top_100_categorical_accuracy: 0.0994 - loss: 33085.1355 - regularization_loss: 0.0000e+00 - total_loss: 33085.1355\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 7s 285ms/step - factorized_top_k/top_1_categorical_accuracy: 9.0000e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0048 - factorized_top_k/top_10_categorical_accuracy: 0.0143 - factorized_top_k/top_50_categorical_accuracy: 0.1040 - factorized_top_k/top_100_categorical_accuracy: 0.2078 - loss: 31012.8495 - regularization_loss: 0.0000e+00 - total_loss: 31012.8495\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 7s 288ms/step - factorized_top_k/top_1_categorical_accuracy: 3.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0079 - factorized_top_k/top_10_categorical_accuracy: 0.0217 - factorized_top_k/top_50_categorical_accuracy: 0.1437 - factorized_top_k/top_100_categorical_accuracy: 0.2676 - loss: 30420.1816 - regularization_loss: 0.0000e+00 - total_loss: 30420.1816\n",
            "Top 3 recommendations for user 42: [b'Rent-a-Kid (1995)' b'House Arrest (1996)'\n",
            " b'Land Before Time III: The Time of the Great Giving (1995) (V)']\n"
          ]
        }
      ],
      "source": [
        "# Create a retrieval model.\n",
        "model = MovieLensModel(user_model, movie_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
        "\n",
        "# Train for 3 epochs.\n",
        "model.fit(ratings.batch(4096), epochs=3)\n",
        "\n",
        "# Use brute-force search to set up retrieval using the trained representations.\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "index.index_from_dataset(\n",
        "    movies.batch(100).map(lambda title: (title, model.movie_model(title))))\n",
        "\n",
        "# Get some recommendations.\n",
        "_, titles = index(np.array([\"42\"]))\n",
        "print(f\"Top 3 recommendations for user 42: {titles[0, :3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeYA79m1DEX"
      },
      "source": [
        "# Multi-task recommenders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk8QEc4sIPMi"
      },
      "source": [
        "In the [basic retrieval tutorial](basic_retrieval) we built a retrieval system using movie watches as positive interaction signals.\n",
        "\n",
        "In many applications, however, there are multiple rich sources of feedback to draw upon. For example, an e-commerce site may record user visits to product pages (abundant, but relatively low signal), image clicks, adding to cart, and, finally, purchases. It may even record post-purchase signals such as reviews and returns.\n",
        "\n",
        "Integrating all these different forms of feedback is critical to building systems that users love to use, and that do not optimize for any one metric at the expense of overall performance.\n",
        "\n",
        "In addition, building a joint model for multiple tasks may produce better results than building a number of task-specific models. This is especially true where some data is abundant (for example, clicks), and some data is sparse (purchases, returns, manual reviews). In those scenarios, a joint model may be able to use representations learned from the abundant task to improve its predictions on the sparse task via a phenomenon known as [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning). For example, [this paper](https://openreview.net/pdf?id=SJxPVcSonN) shows that a model predicting explicit user ratings from sparse user surveys can be substantially improved by adding an auxiliary task that uses abundant click log data.\n",
        "\n",
        "In this tutorial, we are going to build a multi-objective recommender for Movielens, using both implicit (movie watches) and explicit signals (ratings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwrcZeK7x7xI"
      },
      "source": [
        "## Imports\n",
        "\n",
        "\n",
        "Let's first get our imports out of the way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izzoRqkGb2Zc"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxQ_hy7xPH3N"
      },
      "outputs": [],
      "source": [
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "We're going to use the Movielens 100K dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ySWtibjm_6a"
      },
      "outputs": [],
      "source": [
        "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
        "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
        "\n",
        "# Select the basic features.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRHorm8W1yf3"
      },
      "source": [
        "And repeat our preparations for building vocabularies and splitting the data into a train and a test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS0eDfkjnjJL"
      },
      "outputs": [],
      "source": [
        "# Randomly shuffle data and split between train and test.\n",
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "movie_titles = movies.batch(1_000)\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCi-seR86qqa"
      },
      "source": [
        "## A multi-task model\n",
        "\n",
        "There are two critical parts to multi-task recommenders:\n",
        "\n",
        "1. They optimize for two or more objectives, and so have two or more losses.\n",
        "2. They share variables between the tasks, allowing for transfer learning.\n",
        "\n",
        "In this tutorial, we will define our models as before, but instead of having  a single task, we will have two tasks: one that predicts ratings, and one that predicts movie watches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHrk_SLzKCM"
      },
      "source": [
        "The user and movie models are as before:\n",
        "\n",
        "```python\n",
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add 1 to account for the unknown token.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWCwkE5z8QBe"
      },
      "source": [
        "However, now we will have two tasks. The first is the rating task:\n",
        "\n",
        "```python\n",
        "tfrs.tasks.Ranking(\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgQIXEm8UWf"
      },
      "source": [
        "Its goal is to predict the ratings as accurately as possible.\n",
        "\n",
        "The second is the retrieval task:\n",
        "\n",
        "```python\n",
        "tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(\n",
        "        candidates=movies.batch(128)\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNrv7_gakmF"
      },
      "source": [
        "As before, this task's goal is to predict which movies the user will or will not watch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSWw3xuq8mGh"
      },
      "source": [
        "### Putting it together\n",
        "\n",
        "We put it all together in a model class.\n",
        "\n",
        "The new component here is that - since we have two tasks and two losses - we need to decide on how important each loss is. We can do this by giving each of the losses a weight, and treating these weights as hyperparameters. If we assign a large loss weight to the rating task, our model is going to focus on predicting ratings (but still use some information from the retrieval task); if we assign a large loss weight to the retrieval task, it will focus on retrieval instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFSkOAMgzU0K"
      },
      "outputs": [],
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
        "    # We take the loss weights in the constructor: this allows us to instantiate\n",
        "    # several model objects with different loss weights.\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    # User and movie models.\n",
        "    self.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "    self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # A small model to take in user and movie embeddings and predict ratings.\n",
        "    # We can make this as complicated as we want as long as we output a scalar\n",
        "    # as our prediction.\n",
        "    self.rating_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "\n",
        "    # The tasks.\n",
        "    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.movie_model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # The loss weights.\n",
        "    self.rating_weight = rating_weight\n",
        "    self.retrieval_weight = retrieval_weight\n",
        "\n",
        "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\n",
        "    # And pick out the movie features and pass them into the movie model.\n",
        "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "    \n",
        "    return (\n",
        "        user_embeddings,\n",
        "        movie_embeddings,\n",
        "        # We apply the multi-layered rating model to a concatentation of\n",
        "        # user and movie embeddings.\n",
        "        self.rating_model(\n",
        "            tf.concat([user_embeddings, movie_embeddings], axis=1)\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "\n",
        "    ratings = features.pop(\"user_rating\")\n",
        "\n",
        "    user_embeddings, movie_embeddings, rating_predictions = self(features)\n",
        "\n",
        "    # We compute the loss for each task.\n",
        "    rating_loss = self.rating_task(\n",
        "        labels=ratings,\n",
        "        predictions=rating_predictions,\n",
        "    )\n",
        "    retrieval_loss = self.retrieval_task(user_embeddings, movie_embeddings)\n",
        "\n",
        "    # And combine them using the loss weights.\n",
        "    return (self.rating_weight * rating_loss\n",
        "            + self.retrieval_weight * retrieval_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngvn-c0b8lc2"
      },
      "source": [
        "### Rating-specialized model\n",
        "\n",
        "Depending on the weights we assign, the model will encode a different balance of the tasks. Let's start with a model that only considers ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNfB6rYL0VrS"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel(rating_weight=1.0, retrieval_weight=0.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6kjfF1j0iZR"
      },
      "outputs": [],
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NWadH1q0c_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7852c4ad-ba7e-4fb2-fb35-304deee0d62c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "10/10 [==============================] - 8s 572ms/step - root_mean_squared_error: 2.0903 - factorized_top_k/top_1_categorical_accuracy: 2.6250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0024 - factorized_top_k/top_10_categorical_accuracy: 0.0054 - factorized_top_k/top_50_categorical_accuracy: 0.0294 - factorized_top_k/top_100_categorical_accuracy: 0.0589 - loss: 4.0315 - regularization_loss: 0.0000e+00 - total_loss: 4.0315\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 7s 680ms/step - root_mean_squared_error: 1.1531 - factorized_top_k/top_1_categorical_accuracy: 1.8750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0024 - factorized_top_k/top_10_categorical_accuracy: 0.0054 - factorized_top_k/top_50_categorical_accuracy: 0.0297 - factorized_top_k/top_100_categorical_accuracy: 0.0591 - loss: 1.3189 - regularization_loss: 0.0000e+00 - total_loss: 1.3189\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 6s 604ms/step - root_mean_squared_error: 1.1198 - factorized_top_k/top_1_categorical_accuracy: 1.6250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0025 - factorized_top_k/top_10_categorical_accuracy: 0.0055 - factorized_top_k/top_50_categorical_accuracy: 0.0300 - factorized_top_k/top_100_categorical_accuracy: 0.0597 - loss: 1.2479 - regularization_loss: 0.0000e+00 - total_loss: 1.2479\n",
            "5/5 [==============================] - 3s 286ms/step - root_mean_squared_error: 1.1130 - factorized_top_k/top_1_categorical_accuracy: 3.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0030 - factorized_top_k/top_10_categorical_accuracy: 0.0053 - factorized_top_k/top_50_categorical_accuracy: 0.0298 - factorized_top_k/top_100_categorical_accuracy: 0.0595 - loss: 1.2336 - regularization_loss: 0.0000e+00 - total_loss: 1.2336\n",
            "Retrieval top-100 accuracy: 0.059.\n",
            "Ranking RMSE: 1.113.\n"
          ]
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=3)\n",
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lENViv04-i0T"
      },
      "source": [
        "The model does OK on predicting ratings (with an RMSE of around 1.11), but performs poorly at predicting which movies will be watched or not: its accuracy at 100 is almost 4 times worse than a model trained solely to predict watches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPYd9LtE-4Fm"
      },
      "source": [
        "### Retrieval-specialized model\n",
        "\n",
        "Let's now try a model that focuses on retrieval only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfnkGd2G--Qt"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel(rating_weight=0.0, retrieval_weight=1.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCCBdM7U_B11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a9d0c3-9429-4bf3-f601-46e112e51cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "10/10 [==============================] - 6s 495ms/step - root_mean_squared_error: 3.7238 - factorized_top_k/top_1_categorical_accuracy: 7.5000e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0014 - factorized_top_k/top_10_categorical_accuracy: 0.0041 - factorized_top_k/top_50_categorical_accuracy: 0.0473 - factorized_top_k/top_100_categorical_accuracy: 0.1135 - loss: 69818.0291 - regularization_loss: 0.0000e+00 - total_loss: 69818.0291\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 5s 491ms/step - root_mean_squared_error: 3.7495 - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0116 - factorized_top_k/top_10_categorical_accuracy: 0.0268 - factorized_top_k/top_50_categorical_accuracy: 0.1425 - factorized_top_k/top_100_categorical_accuracy: 0.2658 - loss: 67473.2884 - regularization_loss: 0.0000e+00 - total_loss: 67473.2884\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 5s 489ms/step - root_mean_squared_error: 3.7648 - factorized_top_k/top_1_categorical_accuracy: 0.0014 - factorized_top_k/top_5_categorical_accuracy: 0.0180 - factorized_top_k/top_10_categorical_accuracy: 0.0388 - factorized_top_k/top_50_categorical_accuracy: 0.1773 - factorized_top_k/top_100_categorical_accuracy: 0.3050 - loss: 66329.2550 - regularization_loss: 0.0000e+00 - total_loss: 66329.2550\n",
            "5/5 [==============================] - 2s 282ms/step - root_mean_squared_error: 3.7730 - factorized_top_k/top_1_categorical_accuracy: 8.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0097 - factorized_top_k/top_10_categorical_accuracy: 0.0215 - factorized_top_k/top_50_categorical_accuracy: 0.1251 - factorized_top_k/top_100_categorical_accuracy: 0.2350 - loss: 31085.0700 - regularization_loss: 0.0000e+00 - total_loss: 31085.0700\n",
            "Retrieval top-100 accuracy: 0.235.\n",
            "Ranking RMSE: 3.773.\n"
          ]
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=3)\n",
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjM7j7eY_jPh"
      },
      "source": [
        "We get the opposite result: a model that does well on retrieval, but poorly on predicting ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOFwjUus_pLU"
      },
      "source": [
        "### Joint model\n",
        "\n",
        "Let's now train a model that assigns positive weights to both tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xyDbNMf_t8a"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel(rating_weight=1.0, retrieval_weight=1.0)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pZmM_ub_uEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bb9a7b-2997-4f96-eb49-f95cbdfc70cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "10/10 [==============================] - 6s 492ms/step - root_mean_squared_error: 2.5007 - factorized_top_k/top_1_categorical_accuracy: 3.7500e-05 - factorized_top_k/top_5_categorical_accuracy: 0.0014 - factorized_top_k/top_10_categorical_accuracy: 0.0043 - factorized_top_k/top_50_categorical_accuracy: 0.0450 - factorized_top_k/top_100_categorical_accuracy: 0.1102 - loss: 69811.8281 - regularization_loss: 0.0000e+00 - total_loss: 69811.8281\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 5s 492ms/step - root_mean_squared_error: 1.2097 - factorized_top_k/top_1_categorical_accuracy: 9.8750e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0110 - factorized_top_k/top_10_categorical_accuracy: 0.0255 - factorized_top_k/top_50_categorical_accuracy: 0.1385 - factorized_top_k/top_100_categorical_accuracy: 0.2605 - loss: 67481.2713 - regularization_loss: 0.0000e+00 - total_loss: 67481.2713\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 5s 506ms/step - root_mean_squared_error: 1.1200 - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0175 - factorized_top_k/top_10_categorical_accuracy: 0.0380 - factorized_top_k/top_50_categorical_accuracy: 0.1758 - factorized_top_k/top_100_categorical_accuracy: 0.3040 - loss: 66297.9318 - regularization_loss: 0.0000e+00 - total_loss: 66297.9318\n",
            "5/5 [==============================] - 2s 276ms/step - root_mean_squared_error: 1.1312 - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0082 - factorized_top_k/top_10_categorical_accuracy: 0.0220 - factorized_top_k/top_50_categorical_accuracy: 0.1250 - factorized_top_k/top_100_categorical_accuracy: 0.2345 - loss: 31062.8206 - regularization_loss: 0.0000e+00 - total_loss: 31062.8206\n",
            "Retrieval top-100 accuracy: 0.235.\n",
            "Ranking RMSE: 1.131.\n"
          ]
        }
      ],
      "source": [
        "model.fit(cached_train, epochs=3)\n",
        "metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni_rkOsaB3f9"
      },
      "source": [
        "The result is a model that performs roughly as well on both tasks as each specialized model.\n",
        "\n",
        "### Making prediction\n",
        "\n",
        "We can use the trained multitask model to get trained user and movie embeddings, as well as the predicted rating:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXXh-PLaH_Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88815eda-e01b-43ae-9c97-db377fc1f8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted rating:\n",
            "tf.Tensor([[3.40226]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "trained_movie_embeddings, trained_user_embeddings, predicted_rating = model({\n",
        "      \"user_id\": np.array([\"42\"]),\n",
        "      \"movie_title\": np.array([\"Dances with Wolves (1990)\"])\n",
        "  })\n",
        "print(\"Predicted rating:\")\n",
        "print(predicted_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FADp0pUWINTD"
      },
      "source": [
        "While the results here do not show a clear accuracy benefit from a joint model in this case, multi-task learning is in general an extremely useful tool. We can expect better results when we can transfer knowledge from a data-abundant task (such as clicks) to a closely related data-sparse task (such as purchases)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RCvHKuzsPMp5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikhIvrku-i-L"
      },
      "source": [
        "# Deep & Cross Network (DCN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-rOX95bAye4"
      },
      "source": [
        "This tutorial demonstrates how to use Deep & Cross Network (DCN) to effectively learn feature crosses.\n",
        "\n",
        "##Background\n",
        "\n",
        "**What are feature crosses and why are they important?** Imagine that we are building a recommender system to sell a blender to customers. Then, a customer's past purchase history such as `purchased_bananas` and `purchased_cooking_books`, or geographic features, are single features. If one has purchased both bananas **and** cooking books, then this customer will more likely click on the recommended blender. The combination of `purchased_bananas` and `purchased_cooking_books` is referred to as a **feature cross**, which provides additional interaction information beyond the individual features.\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1e8pYZHM1ZSwqBLYVkKDoGg0_2t2UPc2y\" width=\"600\"/>\n",
        "</center>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**What are the challenges in learning feature crosses?** In Web-scale applications, data are mostly categorical, leading to large and sparse feature space. Identifying effective feature crosses in this setting often requires\n",
        "manual feature engineering or exhaustive search. Traditional feed-forward multilayer perceptron (MLP) models are universal function approximators; however, they cannot efficiently approximate even 2nd or 3rd-order feature crosses [[1](https://arxiv.org/pdf/2008.13535.pdf), [2](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/18fa88ad519f25dc4860567e19ab00beff3f01cb.pdf)].\n",
        "\n",
        "**What is Deep & Cross Network (DCN)?** DCN was designed to learn explicit and bounded-degree cross features more effectively. It starts with an input layer (typically an embedding layer), followed by a *cross network* containing multiple cross layers that models explicit feature interactions, and then combines\n",
        "with a *deep network* that models implicit feature interactions.\n",
        "\n",
        "\n",
        "*   Cross Network. This is the core of DCN. It explicitly applies feature crossing at each layer, and the highest\n",
        "polynomial degree increases with layer depth. The following figure shows the $(i+1)$-th cross layer.\n",
        "<div class=\"fig figcenter fighighlight\">\n",
        "<center>\n",
        "  <img src=\"http://drive.google.com/uc?export=view&id=1QvIDptMxixFNp6P4bBqMN4AYAhAIAYQZ\" width=\"50%\" style=\"display:block\">\n",
        "  </center>\n",
        "</div>\n",
        "*   Deep Network. It is a traditional feedforward multilayer perceptron (MLP).\n",
        "\n",
        "The deep network and cross network are then combined to form DCN [[1](https://arxiv.org/pdf/2008.13535.pdf)]. Commonly, we could stack a deep network on top of the cross network (stacked structure); we could also place them in parallel (parallel structure). \n",
        "\n",
        "\n",
        "<div class=\"fig figcenter fighighlight\">\n",
        "<center>\n",
        "  <img src=\"http://drive.google.com/uc?export=view&id=1WtDUCV6b-eetUnWVCAmcPh8mJFut5EUd\" hspace=\"40\" width=\"30%\" style=\"margin: 0px 100px 0px 0px;\">\n",
        "  <img src=\"http://drive.google.com/uc?export=view&id=1xo_twKb847hasfss7JxF0UtFX_rEb4nt\" width=\"20%\">\n",
        "  </center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OlIGoADAhZg"
      },
      "source": [
        "In the following, we will first show the advantage of DCN with a toy example, and then we will walk you through some common ways to utilize DCN using the MovieLen-1M dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az-y_3qxH3gA"
      },
      "source": [
        "Let's first install and import the necessary packages for this colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqsyLA0UHeCl"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHCgOeoHBGbb"
      },
      "source": [
        "## Toy Example\n",
        "To illustrate the benefits of DCN, let's work through a simple example. Suppose we have a dataset where we're trying to model the likelihood of a customer clicking on a blender Ad, with its features and label described as follows.\n",
        "\n",
        "| Features / Label        | Description           | Value Type / Range  |\n",
        "| ------------- |-------------| -----|\n",
        "| $x_1$ = country | the country this customer lives in | Int in [0, 199] |\n",
        "| $x_2$ = bananas | # bananas the customer has purchased |Int in   [0, 23] |\n",
        "| $x_3$ = cookbooks | # cooking books the customer has purchased     |Int in  [0, 5] |\n",
        "| $y$ | the likelihood of clicking on a blender Ad     |   -- |\n",
        "\n",
        "Then, we let the data follow the following underlying distribution:\n",
        "$$y = f(x_1, x_2, x_3) = 0.1x_1 + 0.4x_2+0.7x_3 + 0.1x_1x_2+3.1x_2x_3+0.1x_3^2$$\n",
        "\n",
        "where the likelihood $y$ depends linearly both on features $x_i$'s, but also on multiplicative interactions between the $x_i$'s. In our case, we would say that the likelihood of purchasing a blender ($y$) depends not just on buying bananas ($x_2$) or cookbooks ($x_3$), but also on buying bananas and cookbooks *together* ($x_2x_3$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO6d-0zoHrz8"
      },
      "source": [
        "We can generate the data for this as follows:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fi2ya4P_hab"
      },
      "source": [
        "### Synthetic data generation\n",
        "\n",
        "We first define $f(x_1, x_2, x_3)$ as described above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rT3f6C3GX0u"
      },
      "outputs": [],
      "source": [
        "def get_mixer_data(data_size=100_000, random_seed=42):\n",
        "  # We need to fix the random seed\n",
        "  # to make colab runs repeatable.\n",
        "  rng = np.random.RandomState(random_seed)\n",
        "  country = rng.randint(200, size=[data_size, 1]) / 200.\n",
        "  bananas = rng.randint(24, size=[data_size, 1]) / 24.\n",
        "  cookbooks = rng.randint(6, size=[data_size, 1]) / 6.\n",
        "\n",
        "  x = np.concatenate([country, bananas, cookbooks], axis=1)\n",
        "\n",
        "  # # Create 1st-order terms.\n",
        "  y = 0.1 * country + 0.4 * bananas + 0.7 * cookbooks\n",
        "\n",
        "  # Create 2nd-order cross terms.\n",
        "  y += 0.1 * country * bananas + 3.1 * bananas * cookbooks + (\n",
        "        0.1 * cookbooks * cookbooks)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXtUSs9E3uRG"
      },
      "source": [
        "Let's generate the data that follows the distribution, and split the data into 90% for training and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrQWVYajgmNV"
      },
      "outputs": [],
      "source": [
        "x, y = get_mixer_data()\n",
        "num_train = 90000\n",
        "train_x = x[:num_train]\n",
        "train_y = y[:num_train]\n",
        "eval_x = x[num_train:]\n",
        "eval_y = y[num_train:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MszQC-KJLhVK"
      },
      "source": [
        "### Model construction\n",
        "\n",
        "We're going to try out both cross network and deep network to illustrate the advantage a cross network can bring to recommenders. As the data we just created only contains 2nd-order feature interactions, it would be sufficient to illustrate with a single-layered cross network. If we wanted to model higher-order feature interactions, we could stack multiple cross layers and use a multi-layered cross network. The two models we will be building are:\n",
        "1.   Cross Network with only one cross layer;\n",
        "2.   Deep Network with wider and deeper ReLU layers. \n",
        "\n",
        "We first build a unified model class whose loss is the mean squared error. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwgAH2FTR4Fe"
      },
      "outputs": [],
      "source": [
        "class Model(tfrs.Model):\n",
        "\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self._model = model\n",
        "    self._logit_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    self.task = tfrs.tasks.Ranking(\n",
        "      loss=tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[\n",
        "        tf.keras.metrics.RootMeanSquaredError(\"RMSE\")\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self._model(x)\n",
        "    return self._logit_layer(x)\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    x, labels = features\n",
        "    scores = self(x)\n",
        "\n",
        "    return self.task(\n",
        "        labels=labels,\n",
        "        predictions=scores,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAKBq2QxOdM0"
      },
      "source": [
        "Then, we specify the cross network (with 1 cross layer of size 3) and the ReLU-based DNN (with layer sizes [512, 256, 128]):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwBwSHz_N3pW"
      },
      "outputs": [],
      "source": [
        "crossnet = Model(tfrs.layers.dcn.Cross())\n",
        "deepnet = Model(\n",
        "    tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "    ])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EtaI5lh4X0B"
      },
      "source": [
        "### Model training\n",
        "Now that we have the data and models ready, we are going to train the models. We first shuffle and batch the data to prepare for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6gD-NTF4eoj"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(1000)\n",
        "eval_data = tf.data.Dataset.from_tensor_slices((eval_x, eval_y)).batch(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYm5bmmgPVZu"
      },
      "source": [
        "Then, we define the number of epochs as well as the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFhrC7fV6szW"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbRiVPJtPz-1"
      },
      "source": [
        "Alright, everything is ready now and let's compile and train the models. You could set `verbose=True` if you want to see how the model progresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8ZXXbmKuB8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2092d10-3817-49b6-83c6-57098d6b508c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc040a90950>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "crossnet.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate))\n",
        "crossnet.fit(train_data, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzg3KLKW2sdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793744d1-3ece-4062-a3a8-56c0d2996f08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0409bd090>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "deepnet.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate))\n",
        "deepnet.fit(train_data, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxWfaY6H7Bmp"
      },
      "source": [
        "### Model evaluation\n",
        "We verify the model performance on the evaluation dataset and report the Root Mean Squared Error (RMSE, the lower the better)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4PM-goX6FoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132ce4ce-947f-42db-a3f8-2e943bd6329d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossNet(1 layer) RMSE is 0.0007 using 16 parameters.\n",
            "DeepNet(large) RMSE is 0.0582 using 166401 parameters.\n"
          ]
        }
      ],
      "source": [
        "crossnet_result = crossnet.evaluate(eval_data, return_dict=True, verbose=False)\n",
        "print(f\"CrossNet(1 layer) RMSE is {crossnet_result['RMSE']:.4f} \"\n",
        "      f\"using {crossnet.count_params()} parameters.\")\n",
        "\n",
        "deepnet_result = deepnet.evaluate(eval_data, return_dict=True, verbose=False)\n",
        "print(f\"DeepNet(large) RMSE is {deepnet_result['RMSE']:.4f} \"\n",
        "      f\"using {deepnet.count_params()} parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Ig-Gnm7-JD"
      },
      "source": [
        "We see that the cross network achieved **magnitudes lower RMSE** than a ReLU-based DNN, with **magnitudes fewer parameters**. This has suggested the efficieny of a cross network in learning feaure crosses.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsCsY1-Us-_e"
      },
      "source": [
        "### Model understanding\n",
        "We already know what feature crosses are important in our data, it would be fun to check whether our model has indeed learned the important feature cross. This can be done by visualizing the learned weight matrix in DCN. The weight $W_{ij}$ represents the learned importance of interaction between feature $x_i$ and $x_j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8dga2Qck5IV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "557891f8-2cb0-4823-c1b7-4bfd0e38798b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x648 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "mat = crossnet._model._dense.kernel\n",
        "features = [\"country\", \"purchased_bananas\", \"purchased_cookbooks\"]\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "im = plt.matshow(np.abs(mat.numpy()), cmap=plt.cm.Blues)\n",
        "ax = plt.gca()\n",
        "divider = make_axes_locatable(plt.gca())\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "plt.colorbar(im, cax=cax)\n",
        "cax.tick_params(labelsize=10) \n",
        "_ = ax.set_xticklabels([''] + features, rotation=45, fontsize=10)\n",
        "_ = ax.set_yticklabels([''] + features, fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQHVZTu03qvi"
      },
      "source": [
        "Darker colours represent stronger learned interactions - in this case, it's clear that the model learned that purchasing babanas and cookbooks together is important.\n",
        "\n",
        "If you are interested in trying out more complicated synthetic data, feel free to check out [this paper](https://arxiv.org/pdf/2008.13535.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wU4FcpfHCZM"
      },
      "source": [
        "## Movielens 1M example\n",
        "We now examine the effectiveness of DCN on a real-world dataset: Movielens 1M [[3](https://grouplens.org/datasets/movielens)]. Movielens 1M is a popular dataset for recommendation research. It predicts users' movie ratings given user-related features and movie-related features. We use this dataset to demonstrate some common ways to utilize DCN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rvlem07wfwH"
      },
      "source": [
        "### Data processing\n",
        "\n",
        "The data processing procedure follows a similar procedure as the [basic ranking tutorial](https://www.tensorflow.org/recommenders/examples/basic_ranking)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y_n3EPosR4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf39098-4fb9-41d7-d833-96f414ca344c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:The handle \"movie_lens\" for the MovieLens dataset is deprecated. Prefer using \"movielens\" instead.\n"
          ]
        }
      ],
      "source": [
        "ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_id\": x[\"movie_id\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"],\n",
        "    \"user_gender\": int(x[\"user_gender\"]),\n",
        "    \"user_zip_code\": x[\"user_zip_code\"],\n",
        "    \"user_occupation_text\": x[\"user_occupation_text\"],\n",
        "    \"bucketized_user_age\": int(x[\"bucketized_user_age\"]),\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yb3KxrgSHiF"
      },
      "source": [
        "Next, we randomly split the data into 80% for training and 20% for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5-l91jR_zEo"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRHGa9mESMVz"
      },
      "source": [
        "Then, we create vocabulary for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9qhEcHq_VfI"
      },
      "outputs": [],
      "source": [
        "feature_names = [\"movie_id\", \"user_id\", \"user_gender\", \"user_zip_code\",\n",
        "                 \"user_occupation_text\", \"bucketized_user_age\"]\n",
        "\n",
        "vocabularies = {}\n",
        "\n",
        "for feature_name in feature_names:\n",
        "  vocab = ratings.batch(1_000_000).map(lambda x: x[feature_name])\n",
        "  vocabularies[feature_name] = np.unique(np.concatenate(list(vocab)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eti8kNkPSORk"
      },
      "source": [
        "### Model construction\n",
        "\n",
        "The model architecture we will be building starts with an embedding layer, which is fed into a cross network followed by a deep network. The embedding dimension is set to 32 for all the features. You could also use different embedding sizes for different features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lrDcBjiwnHU"
      },
      "outputs": [],
      "source": [
        "class DCN(tfrs.Model):\n",
        "\n",
        "  def __init__(self, use_cross_layer, deep_layer_sizes, projection_dim=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_dimension = 32\n",
        "\n",
        "    str_features = [\"movie_id\", \"user_id\", \"user_zip_code\",\n",
        "                    \"user_occupation_text\"]\n",
        "    int_features = [\"user_gender\", \"bucketized_user_age\"]\n",
        "\n",
        "    self._all_features = str_features + int_features\n",
        "    self._embeddings = {}\n",
        "\n",
        "    # Compute embeddings for string features.\n",
        "    for feature_name in str_features:\n",
        "      vocabulary = vocabularies[feature_name]\n",
        "      self._embeddings[feature_name] = tf.keras.Sequential(\n",
        "          [tf.keras.layers.StringLookup(\n",
        "              vocabulary=vocabulary, mask_token=None),\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
        "                                     self.embedding_dimension)\n",
        "    ])\n",
        "      \n",
        "    # Compute embeddings for int features.\n",
        "    for feature_name in int_features:\n",
        "      vocabulary = vocabularies[feature_name]\n",
        "      self._embeddings[feature_name] = tf.keras.Sequential(\n",
        "          [tf.keras.layers.IntegerLookup(\n",
        "              vocabulary=vocabulary, mask_value=None),\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\n",
        "                                     self.embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    if use_cross_layer:\n",
        "      self._cross_layer = tfrs.layers.dcn.Cross(\n",
        "          projection_dim=projection_dim,\n",
        "          kernel_initializer=\"glorot_uniform\")\n",
        "    else:\n",
        "      self._cross_layer = None\n",
        "\n",
        "    self._deep_layers = [tf.keras.layers.Dense(layer_size, activation=\"relu\")\n",
        "      for layer_size in deep_layer_sizes]\n",
        "\n",
        "    self._logit_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    self.task = tfrs.tasks.Ranking(\n",
        "      loss=tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError(\"RMSE\")]\n",
        "    )\n",
        "\n",
        "  def call(self, features):\n",
        "    # Concatenate embeddings\n",
        "    embeddings = []\n",
        "    for feature_name in self._all_features:\n",
        "      embedding_fn = self._embeddings[feature_name]\n",
        "      embeddings.append(embedding_fn(features[feature_name]))\n",
        "\n",
        "    x = tf.concat(embeddings, axis=1)\n",
        "\n",
        "    # Build Cross Network\n",
        "    if self._cross_layer is not None:\n",
        "      x = self._cross_layer(x)\n",
        "    \n",
        "    # Build Deep Network\n",
        "    for deep_layer in self._deep_layers:\n",
        "      x = deep_layer(x)\n",
        "\n",
        "    return self._logit_layer(x)\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    labels = features.pop(\"user_rating\")\n",
        "    scores = self(features)\n",
        "    return self.task(\n",
        "        labels=labels,\n",
        "        predictions=scores,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDiRfzwVW9LH"
      },
      "source": [
        "### Model training\n",
        "We shuffle, batch and cache the training and test data. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeFjmfUbgzcS"
      },
      "outputs": [],
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5adSI3yOt2VQ"
      },
      "source": [
        "Let's define a function that runs a model multiple times and returns the model's RMSE mean and standard deviation out of multiple runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTDk3GloquHO"
      },
      "outputs": [],
      "source": [
        "def run_models(use_cross_layer, deep_layer_sizes, projection_dim=None, num_runs=5):\n",
        "  models = []\n",
        "  rmses = []\n",
        "\n",
        "  for i in range(num_runs):\n",
        "    model = DCN(use_cross_layer=use_cross_layer,\n",
        "                deep_layer_sizes=deep_layer_sizes,\n",
        "                projection_dim=projection_dim)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
        "    models.append(model)\n",
        "\n",
        "    model.fit(cached_train, epochs=epochs, verbose=False)\n",
        "    metrics = model.evaluate(cached_test, return_dict=True)\n",
        "    rmses.append(metrics[\"RMSE\"])\n",
        "\n",
        "  mean, stdv = np.average(rmses), np.std(rmses)\n",
        "\n",
        "  return {\"model\": models, \"mean\": mean, \"stdv\": stdv}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRHjQ8g2h2-k"
      },
      "source": [
        "We set some hyper-parameters for the models. Note that these hyper-parameters are set globally for all the models for demonstration purpose. If you want to obtain the best performance for each model, or conduct a fair comparison among models, then we'd suggest you to fine-tune the hyper-parameters. Remember that the model architecture and optimization schemes are intertwined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy3kWb5Dh0E7"
      },
      "outputs": [],
      "source": [
        "epochs = 8\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz3ftiQLXdC0"
      },
      "source": [
        "**DCN (stacked).** We first train a DCN model with a stacked structure, that is, the inputs are fed to a cross network followed by a deep network.\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1X8qoMtIYKJz4yBYifvfw4QpAwrjr70e_\" width=\"140\"/>\n",
        "</center>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiuYPJWhgw3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55424ae-a4d4-416a-8dbf-31a30260de19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:mask_value is deprecated, use mask_token instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:mask_value is deprecated, use mask_token instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 2s 20ms/step - RMSE: 0.9315 - loss: 0.8680 - regularization_loss: 0.0000e+00 - total_loss: 0.8680\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9339 - loss: 0.8726 - regularization_loss: 0.0000e+00 - total_loss: 0.8726\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9326 - loss: 0.8703 - regularization_loss: 0.0000e+00 - total_loss: 0.8703\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9351 - loss: 0.8752 - regularization_loss: 0.0000e+00 - total_loss: 0.8752\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9339 - loss: 0.8730 - regularization_loss: 0.0000e+00 - total_loss: 0.8730\n"
          ]
        }
      ],
      "source": [
        "dcn_result = run_models(use_cross_layer=True,\n",
        "                        deep_layer_sizes=[192, 192])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwTn_UpDX_iO"
      },
      "source": [
        "**Low-rank DCN.** To reduce the training and serving cost, we leverage low-rank techniques to approximate the DCN weight matrices. The rank is passed in through argument `projection_dim`; a smaller `projection_dim` results in a lower cost. Note that `projection_dim` needs to be smaller than (input size)/2 to reduce the cost. In practice, we've observed using low-rank DCN with rank (input size)/4 consistently preserved the accuracy of a full-rank DCN.\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1ZZfUTNdxjGAaAuwNrweKkLJ1PGxMmiCm\" width=\"400\"/>\n",
        "</center>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYxbHI7ZNJX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4f7209-d8ec-44e8-8fbe-b30b23d39fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9307 - loss: 0.8670 - regularization_loss: 0.0000e+00 - total_loss: 0.8670\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9312 - loss: 0.8668 - regularization_loss: 0.0000e+00 - total_loss: 0.8668\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9303 - loss: 0.8666 - regularization_loss: 0.0000e+00 - total_loss: 0.8666\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9337 - loss: 0.8724 - regularization_loss: 0.0000e+00 - total_loss: 0.8724\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9300 - loss: 0.8656 - regularization_loss: 0.0000e+00 - total_loss: 0.8656\n"
          ]
        }
      ],
      "source": [
        "dcn_lr_result = run_models(use_cross_layer=True,\n",
        "                           projection_dim=20,\n",
        "                           deep_layer_sizes=[192, 192])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O5AoNOdaQ80"
      },
      "source": [
        "**DNN.** We train a same-sized DNN model as a reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBPpwD4cGtXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317cffe1-d042-4577-a1fe-335b7ce63e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9464 - loss: 0.8994 - regularization_loss: 0.0000e+00 - total_loss: 0.8994\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9351 - loss: 0.8762 - regularization_loss: 0.0000e+00 - total_loss: 0.8762\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9396 - loss: 0.8843 - regularization_loss: 0.0000e+00 - total_loss: 0.8843\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9362 - loss: 0.8772 - regularization_loss: 0.0000e+00 - total_loss: 0.8772\n",
            "5/5 [==============================] - 0s 4ms/step - RMSE: 0.9377 - loss: 0.8798 - regularization_loss: 0.0000e+00 - total_loss: 0.8798\n"
          ]
        }
      ],
      "source": [
        "dnn_result = run_models(use_cross_layer=False,\n",
        "                        deep_layer_sizes=[192, 192, 192])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBY0ljpl3_k5"
      },
      "source": [
        "We evaluate the model on test data and report the mean and standard deviation out of 5 runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1yj3pp0glEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc55e35-543e-41b5-f2b9-a528d1ce127a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DCN            RMSE mean: 0.9334, stdv: 0.0012\n",
            "DCN (low-rank) RMSE mean: 0.9312, stdv: 0.0013\n",
            "DNN            RMSE mean: 0.9390, stdv: 0.0040\n"
          ]
        }
      ],
      "source": [
        "print(\"DCN            RMSE mean: {:.4f}, stdv: {:.4f}\".format(\n",
        "    dcn_result[\"mean\"], dcn_result[\"stdv\"]))\n",
        "print(\"DCN (low-rank) RMSE mean: {:.4f}, stdv: {:.4f}\".format(\n",
        "    dcn_lr_result[\"mean\"], dcn_lr_result[\"stdv\"]))\n",
        "print(\"DNN            RMSE mean: {:.4f}, stdv: {:.4f}\".format(\n",
        "    dnn_result[\"mean\"], dnn_result[\"stdv\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K076UbT1nnq3"
      },
      "source": [
        "We see that DCN achieved better performance than a same-sized DNN with ReLU layers. Moreover, the low-rank DCN was able to reduce parameters while maintaining the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSF0gNLGX1Za"
      },
      "source": [
        "**More on DCN.** Besides what've been demonstrated above, there are more creative yet practically useful ways to utilize DCN [[1](https://arxiv.org/pdf/2008.13535.pdf)]. \n",
        "\n",
        "*   *DCN with a parallel structure*.  The inputs are fed in parallel to a cross network and a deep network.\n",
        "\n",
        "*   *Concatenating cross layers.* The inputs are fed in parallel to multiple cross layers to capture complementary feature crosses.\n",
        "\n",
        "<div class=\"fig figcenter fighighlight\">\n",
        "<center>\n",
        "  <img src=\"http://drive.google.com/uc?export=view&id=11RpNuj9s0OgSav9TUuGA7v7PuFLL6nVR\" hspace=40 width=\"600\" style=\"display:block;\">\n",
        "  <div class=\"figcaption\">\n",
        "  <b>Left</b>: DCN with a parallel structure; <b>Right</b>: Concatenating cross layers. \n",
        "  </div>\n",
        "  </center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEi9PtCEdyma"
      },
      "source": [
        "### Model understanding\n",
        "\n",
        "The weight matrix $W$ in DCN reveals what feature crosses the model has learned to be important. Recall that in the previous toy example, the importance of interactions between the $i$-th and $j$-th features is captured by the ($i, j$)-th element of $W$.\n",
        "\n",
        "What's a bit different here is that the feature embeddings are of size 32 instead of size 1. Hence, the importance will be characterized by the $(i, j)$-th block\n",
        "$W_{i,j}$ which is of dimension 32 by 32.\n",
        "In the following, we visualize the Frobenius norm [[4](https://en.wikipedia.org/wiki/Matrix_norm)] $||W_{i,j}||_F$ of each block, and a larger norm would suggest higher importance (assuming the features' embeddings are of similar scales).\n",
        "\n",
        "Besides block norm, we could also visualize the entire matrix, or the mean/median/max value of each block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ibaEBJxOoe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "db2b4513-bd24-4692-f24e-f6f73d95088c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x648 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "model = dcn_result[\"model\"][0]\n",
        "mat = model._cross_layer._dense.kernel\n",
        "features = model._all_features\n",
        "\n",
        "block_norm = np.ones([len(features), len(features)])\n",
        "\n",
        "dim = model.embedding_dimension\n",
        "\n",
        "# Compute the norms of the blocks.\n",
        "for i in range(len(features)):\n",
        "  for j in range(len(features)):\n",
        "    block = mat[i * dim:(i + 1) * dim,\n",
        "                j * dim:(j + 1) * dim]\n",
        "    block_norm[i,j] = np.linalg.norm(block, ord=\"fro\")\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "im = plt.matshow(block_norm, cmap=plt.cm.Blues)\n",
        "ax = plt.gca()\n",
        "divider = make_axes_locatable(plt.gca())\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "plt.colorbar(im, cax=cax)\n",
        "cax.tick_params(labelsize=10) \n",
        "_ = ax.set_xticklabels([\"\"] + features, rotation=45, ha=\"left\", fontsize=10)\n",
        "_ = ax.set_yticklabels([\"\"] + features, fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQH-moYd6ZKC"
      },
      "source": [
        "That's all for this colab! We hope that you have enjoyed learning some basics of DCN and common ways to utilize it. If you are interested in learning more, you could check out two relevant papers: [DCN-v1-paper](https://arxiv.org/pdf/1708.05123.pdf), [DCN-v2-paper](https://arxiv.org/pdf/2008.13535.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfAGbq2es2Yn"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "##References\n",
        "[DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](https://arxiv.org/pdf/2008.13535.pdf). \\\n",
        "*Ruoxi Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, Ed Chi. (2020)*\n",
        "\n",
        "\n",
        "[Deep & Cross Network for Ad Click Predictions](https://arxiv.org/pdf/1708.05123.pdf). \\\n",
        "*Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang. (AdKDD 2017)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq9kCbELjzgJ"
      },
      "source": [
        "# Listwise ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4BKWZyB_Hmf"
      },
      "source": [
        "In [the basic ranking tutorial](basic_ranking), we trained a model that can predict ratings for user/movie pairs. The model was trained to minimize the mean squared error of predicted ratings.\n",
        "\n",
        "However, optimizing the model's predictions on individual movies is not necessarily the best method for training ranking models. We do not need ranking models to predict scores with great accuracy. Instead, we care more about the ability of the model to generate an ordered list of items that matches the user's preference ordering.\n",
        "\n",
        "Instead of optimizing the model's predictions on individual query/item pairs, we can optimize the model's ranking of a list as a whole. This method is called _listwise ranking_.\n",
        "\n",
        "In this tutorial, we will use TensorFlow Recommenders to build listwise ranking models. To do so, we will make use of ranking losses and metrics provided by [TensorFlow Ranking](https://github.com/tensorflow/ranking), a TensorFlow package that focuses on [learning to rank](https://www.microsoft.com/en-us/research/publication/learning-to-rank-from-pairwise-approach-to-listwise-approach/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XS680n2n0rL"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "If TensorFlow Ranking is not available in your runtime environment, you can install it using `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr_BrcNMKji6"
      },
      "outputs": [],
      "source": [
        "#!pip install -q tensorflow-recommenders\n",
        "#!pip install -q --upgrade tensorflow-datasets\n",
        "!pip install -q tensorflow-ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPQqa1uYKrw2"
      },
      "source": [
        "We can then import all the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ekaJkcuHsiY"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdTPCz136mvc"
      },
      "outputs": [],
      "source": [
        "import tensorflow_ranking as tfr\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNEB3VRs3bOS"
      },
      "source": [
        "We will continue to use the MovieLens 100K dataset. As before, we load the datasets and keep only the user id, movie title, and user rating features for this tutorial. We also do some houskeeping to prepare our vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-VF30hJn5-3"
      },
      "outputs": [],
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
        "    lambda x: x[\"user_id\"]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIBH-Axc7oqB"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "However, we cannot use the MovieLens dataset for list optimization directly. To perform listwise optimization, we need to have access to a list of movies each user has rated, but each example in the MovieLens 100K dataset contains only the rating of a single movie.\n",
        "\n",
        "To get around this we transform the dataset so that each example contains a user id and a list of movies rated by that user. Some movies in the list will be ranked higher than others; the goal of our model will be to make predictions that match this ordering.\n",
        "\n",
        "To do this, we use the `tfrs.examples.movielens.movielens_to_listwise` helper function. It takes the MovieLens 100K dataset and generates a dataset containing list examples as discussed above. The implementation details can be found in the [source code](https://github.com/tensorflow/recommenders/blob/main/tensorflow_recommenders/examples/movielens.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X99torl5z4Iu"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Split between train and tests sets, as before.\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "# We sample 50 lists for each user for the training data. For each list we\n",
        "# sample 5 movies from the movies the user rated.\n",
        "train = tfrs.examples.movielens.sample_listwise(\n",
        "    train,\n",
        "    num_list_per_user=50,\n",
        "    num_examples_per_list=5,\n",
        "    seed=42\n",
        ")\n",
        "test = tfrs.examples.movielens.sample_listwise(\n",
        "    test,\n",
        "    num_list_per_user=1,\n",
        "    num_examples_per_list=5,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zuAfGrgCBJP"
      },
      "source": [
        "We can inspect an example from the training data. The example includes a user id, a list of 10 movie ids, and their ratings by the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO52eZqOzOUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06868c2b-20fa-4722-b35a-4e2e76d00cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'movie_title': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
            "array([b'Postman, The (1997)', b'Liar Liar (1997)', b'Contact (1997)',\n",
            "       b'Welcome To Sarajevo (1997)',\n",
            "       b'I Know What You Did Last Summer (1997)'], dtype=object)>,\n",
            " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'681'>,\n",
            " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([4., 5., 1., 4., 1.], dtype=float32)>}\n"
          ]
        }
      ],
      "source": [
        "for example in train.take(1):\n",
        "  pprint.pprint(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM3uu5hgN4-v"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "We will train the same model with three different losses: \n",
        "\n",
        "- mean squared error,\n",
        "- pairwise hinge loss, and\n",
        "- a listwise ListMLE loss. \n",
        "\n",
        "These three losses correspond to pointwise, pairwise, and listwise optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXmqkuAShHO7"
      },
      "source": [
        "To evaluate the model we use [normalized discounted cumulative gain (NDCG)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG). NDCG measures a predicted ranking by taking a weighted sum of the actual rating of each candidate. The ratings of movies that are ranked lower by the model would be discounted more. As a result, a good model that ranks highly-rated movies on top would have a high NDCG result. Since this metric takes the ranked position of each candidate into account, it is a listwise metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1RTt67fhR52"
      },
      "outputs": [],
      "source": [
        "class RankingModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, loss):\n",
        "    super().__init__()\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    # Compute embeddings for users.\n",
        "    self.user_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute embeddings for movies.\n",
        "    self.movie_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_titles),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 2, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute predictions.\n",
        "    self.score_model = tf.keras.Sequential([\n",
        "      # Learn multiple dense layers.\n",
        "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "      # Make rating predictions in the final layer.\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    self.task = tfrs.tasks.Ranking(\n",
        "      loss=loss,\n",
        "      metrics=[\n",
        "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
        "        tf.keras.metrics.RootMeanSquaredError()\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  def call(self, features):\n",
        "    # We first convert the id features into embeddings.\n",
        "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
        "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
        "\n",
        "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
        "    # tensor.\n",
        "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
        "    \n",
        "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
        "    # them into the ranking model. To do so, we need to reshape the user\n",
        "    # embeddings to match the shape of movie embeddings.\n",
        "    list_length = features[\"movie_title\"].shape[1]\n",
        "    user_embedding_repeated = tf.repeat(\n",
        "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
        "\n",
        "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
        "    # predictions.\n",
        "    concatenated_embeddings = tf.concat(\n",
        "        [user_embedding_repeated, movie_embeddings], 2)\n",
        "    \n",
        "    return self.score_model(concatenated_embeddings)\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    labels = features.pop(\"user_rating\")\n",
        "\n",
        "    scores = self(features)\n",
        "\n",
        "    return self.task(\n",
        "        labels=labels,\n",
        "        predictions=tf.squeeze(scores, axis=-1),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcTElTWbOImt"
      },
      "source": [
        "## Training the models\n",
        "\n",
        "We can now train each of the three models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U530Yk-s-g9"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQNnD7DNTkYC"
      },
      "source": [
        "### Mean squared error model\n",
        "\n",
        "This model is very similar to the model in [the basic ranking tutorial](basic_ranking). We train the model to minimize the mean squared error between the actual ratings and predicted ratings. Therefore, this loss is computed individually for each movie and the training is pointwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0lq0Nq7_xTW"
      },
      "outputs": [],
      "source": [
        "mse_model = RankingModel(tf.keras.losses.MeanSquaredError())\n",
        "mse_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NBl543nRtIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf46e0f1-1229-4aac-c1fd-147e2b5846c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbefc25c9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "mse_model.fit(cached_train, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DHphbdxUS7l"
      },
      "source": [
        "### Pairwise hinge loss model\n",
        "\n",
        "By minimizing the pairwise hinge loss, the model tries to maximize the difference between the model's predictions for a highly rated item and a low rated item: the bigger that difference is, the lower the model loss. However, once the difference is large enough, the loss becomes zero, stopping the model from further optimizing this particular pair and letting it focus on other pairs that are incorrectly ranked\n",
        "\n",
        "This loss is not computed for individual movies, but rather for pairs of movies. Hence the training using this loss is pairwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px_wZPxCOrBt"
      },
      "outputs": [],
      "source": [
        "hinge_model = RankingModel(tfr.keras.losses.PairwiseHingeLoss())\n",
        "hinge_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqbd9aDXO6mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327946ce-e51e-4f05-fb3b-3e8bac750895"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbf2e1d50d0>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "hinge_model.fit(cached_train, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79_Y2cuUal-"
      },
      "source": [
        "### Listwise model\n",
        "\n",
        "The `ListMLE` loss from TensorFlow Ranking expresses list maximum likelihood estimation. To calculate the ListMLE loss, we first use the user ratings to generate an optimal ranking. We then calculate the likelihood of each candidate being out-ranked by any item below it in the optimal ranking using the predicted scores. The model tries to minimize such likelihood to ensure highly rated candidates are not out-ranked by low rated candidates. You can learn more about the details of ListMLE in section 2.2 of the paper [Position-aware ListMLE: A Sequential Learning Process](http://auai.org/uai2014/proceedings/individuals/164.pdf).\n",
        "\n",
        "Note that since the likelihood is computed with respect to a candidate and all candidates below it in the optimal ranking, the loss is not pairwise but listwise. Hence the training uses list optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IO8N2JQvASN"
      },
      "outputs": [],
      "source": [
        "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
        "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aA0lqGovDrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d3581d-0cc8-4b78-fac7-d093b5a52f11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbefc988d50>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "listwise_model.fit(cached_train, epochs=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5y0ucbFSEZi"
      },
      "source": [
        "## Comparing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfPgRvnXSJwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0cc80f-bcfc-4682-f3b5-61c6fb3bd0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 387ms/step - ndcg_metric: 0.9053 - root_mean_squared_error: 0.9671 - loss: 0.9354 - regularization_loss: 0.0000e+00 - total_loss: 0.9354\n",
            "NDCG of the MSE Model: 0.9053\n"
          ]
        }
      ],
      "source": [
        "mse_model_result = mse_model.evaluate(cached_test, return_dict=True)\n",
        "print(\"NDCG of the MSE Model: {:.4f}\".format(mse_model_result[\"ndcg_metric\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwTaEJ6JPF9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92eb60ba-027f-4137-9155-5e32944b8cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 454ms/step - ndcg_metric: 0.9058 - root_mean_squared_error: 3.8332 - loss: 1.0180 - regularization_loss: 0.0000e+00 - total_loss: 1.0180\n",
            "NDCG of the pairwise hinge loss model: 0.9058\n"
          ]
        }
      ],
      "source": [
        "hinge_model_result = hinge_model.evaluate(cached_test, return_dict=True)\n",
        "print(\"NDCG of the pairwise hinge loss model: {:.4f}\".format(hinge_model_result[\"ndcg_metric\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR8xQs6BSO0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab6d713-43ac-4108-e289-a06e550afdcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fbf0bd38cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fbf0bd38cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 429ms/step - ndcg_metric: 0.9072 - root_mean_squared_error: 2.7254 - loss: 4.5402 - regularization_loss: 0.0000e+00 - total_loss: 4.5402\n",
            "NDCG of the ListMLE model: 0.9072\n"
          ]
        }
      ],
      "source": [
        "listwise_model_result = listwise_model.evaluate(cached_test, return_dict=True)\n",
        "print(\"NDCG of the ListMLE model: {:.4f}\".format(listwise_model_result[\"ndcg_metric\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XeWqIq4O1xo"
      },
      "source": [
        "Of the three models, the model trained using ListMLE has the highest NDCG metric. This result shows how listwise optimization can be used to train ranking models and can potentially produce models that perform better than models optimized in a pointwise or pairwise fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nbt5BVJMSC79"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyskSj38SDKN"
      },
      "source": [
        "# Efficient serving using SCANN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlFcUNXT7hSF"
      },
      "source": [
        "[Retrieval models](https://www.tensorflow.org/recommenders/examples/basic_retrieval) are often built to surface a handful of top candidates out of millions or even hundreds of millions of candidates. To be able to react to the user's context and behaviour, they need to be able to do this on the fly, in a matter of milliseconds.\n",
        "\n",
        "Approximate nearest neighbour search (ANN) is the technology that makes this possible. In this tutorial, we'll show how to use ScaNN - a state of the art nearest neighbour retrieval package - to seamlessly scale TFRS retrieval to millions of items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_s_2UgUWA9u"
      },
      "source": [
        "## What is ScaNN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSvmiDQPsGmb"
      },
      "source": [
        "ScaNN is a library from Google Research that performs dense vector similarity search at large scale. Given a database of candidate embeddings, ScaNN indexes these embeddings in a manner that allows them to be rapidly searched at inference time. ScaNN uses state of the art vector compression techniques and carefully implemented algorithms to achieve the best speed-accuracy tradeoff. It can greatly outperform brute force search while sacrificing little in terms of accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTpnORU7WEPD"
      },
      "source": [
        "## Building a ScaNN-powered model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXEZ3lZnWIVh"
      },
      "source": [
        "To try out ScaNN in TFRS, we'll build a simple MovieLens retrieval model, just as we did in the [basic retrieval](https://www.tensorflow.org/recommenders/examples/basic_retrieval) tutorial. If you have followed that tutorial, this section will be familiar and can safely be skipped.\n",
        "\n",
        "To start, install TFRS and TensorFlow Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD2hiRviCxFE"
      },
      "outputs": [],
      "source": [
        "#!pip install -q tensorflow-recommenders\n",
        "#!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEbc-66nDJzc"
      },
      "source": [
        "We also need to install `scann`: it's an optional dependency of TFRS, and so needs to be installed separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daEivxsJDO0Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q scann"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDe054pgDQdp"
      },
      "source": [
        "Set up all the necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzEUhX3ASDKO"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Text\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBlRng1QSDKO"
      },
      "outputs": [],
      "source": [
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfmRuUgJWlEQ"
      },
      "source": [
        "And load the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMo2La9JSDKP"
      },
      "outputs": [],
      "source": [
        "# Load the MovieLens 100K data.\n",
        "ratings = tfds.load(\n",
        "    \"movielens/100k-ratings\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "# Get the ratings data.\n",
        "ratings = (ratings\n",
        "           # Retain only the fields we need.\n",
        "           .map(lambda x: {\"user_id\": x[\"user_id\"], \"movie_title\": x[\"movie_title\"]})\n",
        "           # Cache for efficiency.\n",
        "           .cache(tempfile.NamedTemporaryFile().name)\n",
        ")\n",
        "\n",
        "# Get the movies data.\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "movies = (movies\n",
        "          # Retain only the fields we need.\n",
        "          .map(lambda x: x[\"movie_title\"])\n",
        "          # Cache for efficiency.\n",
        "          .cache(tempfile.NamedTemporaryFile().name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiVuNZ-lWv0R"
      },
      "source": [
        "Before we can build a model, we need to set up the user and movie vocabularies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw-iQKBBajnz"
      },
      "outputs": [],
      "source": [
        "user_ids = ratings.map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids.batch(1000))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRbZCvWHWzPU"
      },
      "source": [
        "We'll also set up the training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqV8p7N8CrEg"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok3-kzr1bI7U"
      },
      "source": [
        "### Model definition\n",
        "\n",
        "Just as in the [basic retrieval](https://www.tensorflow.org/recommenders/examples/basic_retrieval) tutorial, we build a simple two-tower model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX_j4pEVbKIS"
      },
      "outputs": [],
      "source": [
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    # Set up a model for representing movies.\n",
        "    self.movie_model = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_movie_titles, mask_token=None),\n",
        "      # We add an additional embedding to account for unknown tokens.\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Set up a model for representing users.\n",
        "    self.user_model = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "        # We add an additional embedding to account for unknown tokens.\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Set up a task to optimize the model and compute metrics.\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "      metrics=tfrs.metrics.FactorizedTopK(\n",
        "        candidates=movies.batch(128).cache().map(self.movie_model)\n",
        "      )\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\n",
        "    # And pick out the movie features and pass them into the movie model,\n",
        "    # getting embeddings back.\n",
        "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "\n",
        "    return self.task(user_embeddings, positive_movie_embeddings, compute_metrics=not training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtO3lKR_XKkw"
      },
      "source": [
        "### Fitting and evaluation\n",
        "\n",
        "A TFRS model is just a Keras model. We can compile it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOGTdwAAbuB6"
      },
      "outputs": [],
      "source": [
        "model = MovielensModel()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rGLyo-XXPmX"
      },
      "source": [
        "Estimate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf_E4dIMcGnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a0a133-9034-43e2-ba0d-2be63f23785f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "10/10 [==============================] - 1s 33ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 69808.9688 - regularization_loss: 0.0000e+00 - total_loss: 69808.9688\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 1s 35ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 67485.8835 - regularization_loss: 0.0000e+00 - total_loss: 67485.8835\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 1s 34ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 66311.9581 - regularization_loss: 0.0000e+00 - total_loss: 66311.9581\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbf1379bad0>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "model.fit(train.batch(8192), epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xymbWgVXSrT"
      },
      "source": [
        "And evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMlIj741cIT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ce86eb-033b-4865-ec00-a3312d28cfb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7fbf0d8f4200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7fbf0d8f4200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 394ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0094 - factorized_top_k/top_10_categorical_accuracy: 0.0226 - factorized_top_k/top_50_categorical_accuracy: 0.1264 - factorized_top_k/top_100_categorical_accuracy: 0.2365 - loss: 49466.8750 - regularization_loss: 0.0000e+00 - total_loss: 49466.8750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factorized_top_k/top_100_categorical_accuracy': 0.23645000159740448,\n",
              " 'factorized_top_k/top_10_categorical_accuracy': 0.022600000724196434,\n",
              " 'factorized_top_k/top_1_categorical_accuracy': 0.0010499999625608325,\n",
              " 'factorized_top_k/top_50_categorical_accuracy': 0.12635000050067902,\n",
              " 'factorized_top_k/top_5_categorical_accuracy': 0.009399999864399433,\n",
              " 'loss': 28242.83203125,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 28242.83203125}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "model.evaluate(test.batch(8192), return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RbHiBWqsFmf"
      },
      "source": [
        "## Approximate prediction\n",
        "\n",
        "The most straightforward way of retrieving top candidates in response to a query is to do it via brute force: compute user-movie scores for all possible movies, sort them, and pick a couple of top recommendations.\n",
        "\n",
        "In TFRS, this is accomplished via the `BruteForce` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_L2yAPjpHsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481f3223-8fc6-4407-edf2-debc5edbe93c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7fbf147ddf90>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "brute_force = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "brute_force.index_from_dataset(\n",
        "    movies.batch(128).map(lambda title: (title, model.movie_model(title)))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzoNR28vXw7o"
      },
      "source": [
        "Once created and populated with candidates (via the `index` method), we can call it to get predictions out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBo1Nu0Grife",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ee6da5-190e-4c77-ef2d-c199cf8ab6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top recommendations: [b'Homeward Bound: The Incredible Journey (1993)'\n",
            " b\"Kid in King Arthur's Court, A (1995)\" b'Rudy (1993)']\n"
          ]
        }
      ],
      "source": [
        "# Get predictions for user 42.\n",
        "_, titles = brute_force(np.array([\"42\"]), k=3)\n",
        "\n",
        "print(f\"Top recommendations: {titles[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzNECPifr6i6"
      },
      "source": [
        "On a small dataset of under 1000 movies, this is very fast:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w57iyu7Ir87Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba1fb7c-8f02-47d1-ee00-b2a288eebd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 1.37 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit _, titles = brute_force(np.array([\"42\"]), k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2AjJsdrsClR"
      },
      "source": [
        "But what happens if we have more candidates - millions instead of thousands?\n",
        "\n",
        "We can simulate this by indexing all of our movies multiple times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AapJk84csTqV"
      },
      "outputs": [],
      "source": [
        "# Construct a dataset of movies that's 1,000 times larger. We \n",
        "# do this by adding several million dummy movie titles to the dataset.\n",
        "lots_of_movies = tf.data.Dataset.concatenate(\n",
        "    movies.batch(4096),\n",
        "    movies.batch(4096).repeat(1_000).map(lambda x: tf.zeros_like(x))\n",
        ")\n",
        "\n",
        "# We also add lots of dummy embeddings by randomly perturbing\n",
        "# the estimated embeddings for real movies.\n",
        "lots_of_movies_embeddings = tf.data.Dataset.concatenate(\n",
        "    movies.batch(4096).map(model.movie_model),\n",
        "    movies.batch(4096).repeat(1_000)\n",
        "      .map(lambda x: model.movie_model(x))\n",
        "      .map(lambda x: x * tf.random.uniform(tf.shape(x)))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viCLP9qSYBQh"
      },
      "source": [
        "We can build a `BruteForce` index on this larger dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfY62oQbYA3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f02e32-8e33-4081-9b7d-e29ba7951e64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x7fbf14740fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "brute_force_lots = tfrs.layers.factorized_top_k.BruteForce()\n",
        "brute_force_lots.index_from_dataset(\n",
        "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrkMt8O_xm-s"
      },
      "source": [
        "The recommendations are still the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9fIYUeYxjki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a7e125-0af0-428f-f2fe-ee94ea12bc47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top recommendations: [b'Homeward Bound: The Incredible Journey (1993)'\n",
            " b\"Kid in King Arthur's Court, A (1995)\" b'Rudy (1993)']\n"
          ]
        }
      ],
      "source": [
        "_, titles = brute_force_lots(model.user_model(np.array([\"42\"])), k=3)\n",
        "\n",
        "print(f\"Top recommendations: {titles[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwF25ZzdseX8"
      },
      "source": [
        "But they take much longer. With a candidate set of 1 million movies, brute force prediction becomes quite slow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oetK_wNxsdw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7302b1-6e3b-4bd5-eeab-f821633331d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 3.7 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit _, titles = brute_force_lots(model.user_model(np.array([\"42\"])), k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKF9yEeotbXQ"
      },
      "source": [
        "As the number of candidate grows, the amount of time needed grows linearly: with 10 million candidates, serving top candidates would take 250 milliseconds. This is clearly too slow for a live service.\n",
        "\n",
        "This is where approximate mechanisms come in.\n",
        "\n",
        "Using ScaNN in TFRS is accomplished via the `tfrs.layers.factorized_top_k.ScaNN` layer. It follow the same interface as the other top k layers:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scann"
      ],
      "metadata": {
        "id": "KvERkxvdSc_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT9ptx_1Ppxt",
        "outputId": "85a74a99-f005-4faa-a365-416268545b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scann in /usr/local/lib/python3.7/dist-packages (1.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scann) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scann) (1.15.0)\n",
            "Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from scann) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (1.0.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (14.0.1)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (0.25.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (0.2.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (1.46.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->scann) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->scann) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->scann) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (3.3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->scann) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLgPmA90sbDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561c5480-b83d-4fa3-b6e7-9bada67c9606"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7fbf1467d5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "from scann import scann_ops\n",
        "\n",
        "scann = tfrs.layers.factorized_top_k.ScaNN(num_reordering_candidates=100)\n",
        "scann.index_from_dataset(\n",
        "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRI-qv7S2h97"
      },
      "source": [
        "The recommendations are (approximately!) the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCkRn1VnxuXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe85007b-d7f3-4204-8bac-1ee0fc0e6d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top recommendations: [b'Homeward Bound: The Incredible Journey (1993)'\n",
            " b\"Kid in King Arthur's Court, A (1995)\" b'Rudy (1993)']\n"
          ]
        }
      ],
      "source": [
        "_, titles = scann(model.user_model(np.array([\"42\"])), k=3)\n",
        "\n",
        "print(f\"Top recommendations: {titles[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW1oBtcC2mb1"
      },
      "source": [
        "But they are much, much faster to compute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooJsLhpWstlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41f8de6-037c-4954-a32f-741eaf14b795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 19.6 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit _, titles = scann(model.user_model(np.array([\"42\"])), k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOYk0zi12q-0"
      },
      "source": [
        "In this case, we can retrieve the top 3 movies out of a set of ~1 million in around 2 milliseconds: 15 times faster than by computing the best candidates via brute force. The advantage of approximate methods grows even larger for larger datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE7eL7ZzDKtl"
      },
      "source": [
        "## Evaluating the approximation\n",
        "\n",
        "When using approximate top K retrieval mechanisms (such as ScaNN), speed of retrieval often comes at the expense of accuracy. To understand this trade-off, it's important to measure the model's evaluation metrics when using ScaNN, and to compare them with the baseline.\n",
        "\n",
        "Fortunately, TFRS makes this easy. We simply override the metrics on the retrieval task with metrics using ScaNN, re-compile the model, and run evaluation.\n",
        "\n",
        "To make the comparison, let's first run baseline results. We still need to override our metrics to make sure they are using the enlarged candidate set rather than the original set of movies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZtJRQqBep_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da76427-eb41-4b2f-a58f-e8b030f38e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16min 1s, sys: 5.59 s, total: 16min 6s\n",
            "Wall time: 4min 7s\n"
          ]
        }
      ],
      "source": [
        "# Override the existing streaming candidate source.\n",
        "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
        "    candidates=lots_of_movies_embeddings\n",
        ")\n",
        "# Need to recompile the model for the changes to take effect.\n",
        "model.compile()\n",
        "\n",
        "%time baseline_result = model.evaluate(test.batch(8192), return_dict=True, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHFzcS5cQtB_"
      },
      "source": [
        "We can do the same using ScaNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T-YxOqoKMje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab398ca1-c18c-4983-9aca-1919209b9023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.54 s, sys: 217 ms, total: 5.76 s\n",
            "Wall time: 2.39 s\n"
          ]
        }
      ],
      "source": [
        "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
        "    candidates=scann\n",
        ")\n",
        "model.compile()\n",
        "\n",
        "# We can use a much bigger batch size here because ScaNN evaluation\n",
        "# is more memory efficient.\n",
        "%time scann_result = model.evaluate(test.batch(8192), return_dict=True, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0gnjcUUZ6-v"
      },
      "source": [
        "ScaNN based evaluation is much, much quicker: it's over ten times faster! This advantage is going to grow even larger for bigger datasets, and so for large datasets it may be prudent to always run ScaNN-based evaluation to improve model development velocity.\n",
        "\n",
        "But how about the results? Fortunately, in this case the results are almost the same:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcXUbZx3Fq4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cecb48-79ef-4d89-b195-70e7fc94090d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brute force top-100 accuracy: 0.15\n",
            "ScaNN top-100 accuracy:       0.27\n"
          ]
        }
      ],
      "source": [
        "print(f\"Brute force top-100 accuracy: {baseline_result['factorized_top_k/top_100_categorical_accuracy']:.2f}\")\n",
        "print(f\"ScaNN top-100 accuracy:       {scann_result['factorized_top_k/top_100_categorical_accuracy']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2UJR-5nZ6YT"
      },
      "source": [
        "This suggests that on this artificial datase,  there is little loss from the approximation. In general, all approximate methods exhibit speed-accuracy tradeoffs. To understand this in more depth you can check out Erik Bernhardsson's [ANN benchmarks](https://github.com/erikbern/ann-benchmarks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jdPPOlV3JOr"
      },
      "source": [
        "## Deploying the approximate model\n",
        "\n",
        "The `ScaNN`-based model is fully integrated into TensorFlow models, and serving it is as easy as serving any other TensorFlow model.\n",
        "\n",
        "We can save it as a `SavedModel` object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKxXVfJBLbiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25308d70-a827-4d6f-b639-5c1889318de7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ConcatenateDataset element_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "lots_of_movies_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnVI_6N53WU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5db9fae-2395-402c-b6e1-9da67e597fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpsmxiitdz/model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpsmxiitdz/model/assets\n"
          ]
        }
      ],
      "source": [
        "# We re-index the ScaNN layer to include the user embeddings in the same model.\n",
        "# This way we can give the saved model raw features and get valid predictions\n",
        "# back.\n",
        "scann = tfrs.layers.factorized_top_k.ScaNN(model.user_model, num_reordering_candidates=1000)\n",
        "scann.index_from_dataset(\n",
        "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
        ")\n",
        "\n",
        "# Need to call it to set the shapes.\n",
        "_ = scann(np.array([\"42\"]))\n",
        "\n",
        "with tempfile.TemporaryDirectory() as tmp:\n",
        "  path = os.path.join(tmp, \"model\")\n",
        "  tf.saved_model.save(\n",
        "      scann,\n",
        "      path,\n",
        "      options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
        "  )\n",
        "\n",
        "  loaded = tf.saved_model.load(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5vDZjro4lXG"
      },
      "source": [
        "and then load it and serve, getting exactly the same results back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXm8smCt3iFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d15dc51-86dc-4140-e69c-f451cd1d15ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top recommendations: [b'Homeward Bound: The Incredible Journey (1993)'\n",
            " b\"Kid in King Arthur's Court, A (1995)\" b'Rudy (1993)']\n"
          ]
        }
      ],
      "source": [
        "_, titles = loaded(tf.constant([\"42\"]))\n",
        "\n",
        "print(f\"Top recommendations: {titles[0][:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Doal2ETqU4"
      },
      "source": [
        "The resulting model can be served in any Python service that has TensorFlow and ScaNN installed.\n",
        "\n",
        "It can also be served using a customized version of TensorFlow Serving, available as a Docker container on [Docker Hub](https://hub.docker.com/r/google/tf-serving-scann). You can also build the image yourself from the [Dockerfile](https://github.com/google-research/google-research/tree/master/scann/tf_serving)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gQsvn5PYbR-"
      },
      "source": [
        "## Tuning ScaNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "918uqacB7sNH"
      },
      "source": [
        "Now let's look into tuning our ScaNN layer to get a better performance/accuracy tradeoff. In order to do this effectively, we first need to measure our baseline performance and accuracy.\n",
        "\n",
        "From above, we already have a measurement of our model's latency for processing a single (non-batched) query (although note that a fair amount of this latency is from non-ScaNN components of the model).\n",
        "\n",
        "Now we need to investigate ScaNN's accuracy, which we measure through recall. A recall@k of x% means that if we use brute force to retrieve the true top k neighbors, and compare those results to using ScaNN to also retrieve the top k neighbors, x% of ScaNN's results are in the true brute force results. Let's compute the recall for the current ScaNN searcher.\n",
        "\n",
        "First, we need to generate the brute force, ground truth top-k:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgf_QuP-8EXb"
      },
      "outputs": [],
      "source": [
        "# Process queries in groups of 1000; processing them all at once with brute force\n",
        "# may lead to out-of-memory errors, because processing a batch of q queries against\n",
        "# a size-n dataset takes O(nq) space with brute force.\n",
        "titles_ground_truth = tf.concat([\n",
        "  brute_force_lots(queries, k=10)[1] for queries in\n",
        "  test.batch(1000).map(lambda x: model.user_model(x[\"user_id\"]))\n",
        "], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSZkWESc856P"
      },
      "source": [
        "Our variable `titles_ground_truth` now contains the top-10 movie recommendations returned by brute-force retrieval. Now we can compute the same recommendations when using ScaNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUKtdf1X87mP"
      },
      "outputs": [],
      "source": [
        "# Get all user_id's as a 1d tensor of strings\n",
        "test_flat = np.concatenate(list(test.map(lambda x: x[\"user_id\"]).batch(1000).as_numpy_iterator()), axis=0)\n",
        "\n",
        "# ScaNN is much more memory efficient and has no problem processing the whole\n",
        "# batch of 20000 queries at once.\n",
        "_, titles = scann(test_flat, k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsTDiAZ9F6h"
      },
      "source": [
        "Next, we define our function that computes recall. For each query, it counts how many results are in the intersection of the brute force and the ScaNN results and divides this by the number of brute force results. The average of this quantity over all queries is our recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCtBew2C9Gv0"
      },
      "outputs": [],
      "source": [
        "def compute_recall(ground_truth, approx_results):\n",
        "  return np.mean([\n",
        "      len(np.intersect1d(truth, approx)) / len(truth)\n",
        "      for truth, approx in zip(ground_truth, approx_results)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdxlKua9JR2"
      },
      "source": [
        "This gives us baseline recall@10 with the current ScaNN config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMi4VtJD9K9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88cffd8e-11fa-438b-92aa-056c93d11b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.931\n"
          ]
        }
      ],
      "source": [
        "print(f\"Recall: {compute_recall(titles_ground_truth, titles):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKpgkNseYWW8"
      },
      "source": [
        "We can also measure the baseline latency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81mO-GS4VJLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5d0622-f743-463c-e0df-844db26c4778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 19.5 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit -n 1000 scann(np.array([\"42\"]), k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UICnYQln9PAq"
      },
      "source": [
        "Let's see if we can do better!\n",
        "\n",
        "To do this, we need a model of how ScaNN's tuning knobs affect performance. Our current model uses ScaNN's tree-AH algorithm. This algorithm partitions the database of embeddings (the \"tree\") and then scores the most promising of these partitions using AH, which is a highly optimized approximate distance computation routine.\n",
        "\n",
        "The default parameters for TensorFlow Recommenders' ScaNN Keras layer sets `num_leaves=100` and `num_leaves_to_search=10`. This means our database is partitioned into 100 disjoint subsets, and the 10 most promising of these partitions is scored with AH. This means 10/100=10% of the dataset is being searched with AH.\n",
        "\n",
        "If we have, say, `num_leaves=1000` and `num_leaves_to_search=100`, we would also be searching 10% of the database with AH. However, in comparison to the previous setting, the 10% we would search will contain higher-quality candidates, because a higher `num_leaves` allows us to make finer-grained decisions about what parts of the dataset are worth searching.\n",
        "\n",
        "It's no surprise then that with `num_leaves=1000` and `num_leaves_to_search=100` we get significantly higher recall:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq6L1Qtl9Qan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa388f38-65df-4b35-8380-5a9c0e03dd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.965\n"
          ]
        }
      ],
      "source": [
        "scann2 = tfrs.layers.factorized_top_k.ScaNN(\n",
        "    model.user_model, \n",
        "    num_leaves=1000,\n",
        "    num_leaves_to_search=100,\n",
        "    num_reordering_candidates=1000)\n",
        "scann2.index_from_dataset(\n",
        "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
        ")\n",
        "\n",
        "_, titles2 = scann2(test_flat, k=10)\n",
        "\n",
        "print(f\"Recall: {compute_recall(titles_ground_truth, titles2):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2WR8zPH9TtW"
      },
      "source": [
        "However, as a tradeoff, our latency has also increased. This is because the partitioning step has gotten more expensive; `scann` picks the top 10 of 100 partitions while `scann2` picks the top 100 of 1000 partitions. The latter can be more expensive because it involves looking at 10 times as many partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po0kb4Mf9VhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881ddb50-9f78-4217-d190-3c3a6d6218c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 19.6 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit -n 1000 scann2(np.array([\"42\"]), k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCDzY0sc9Zgc"
      },
      "source": [
        "In general, tuning ScaNN search is about picking the right tradeoffs. Each individual parameter change generally won't make search both faster and more accurate; our goal is to tune the parameters to optimally trade off between these two conflicting goals.\n",
        "\n",
        "In our case, `scann2` significantly improved recall over `scann` at some cost in latency. Can we dial back some other knobs to cut down on latency, while preserving most of our recall advantage?\n",
        "\n",
        "Let's try searching 70/1000=7% of the dataset with AH, and only rescoring the final 400 candidates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBp8Yvdj9pMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "698e8702-a7ec-4f66-f0a5-72e4e8cfc1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.958\n"
          ]
        }
      ],
      "source": [
        "scann3 = tfrs.layers.factorized_top_k.ScaNN(\n",
        "    model.user_model,\n",
        "    num_leaves=1000,\n",
        "    num_leaves_to_search=70,\n",
        "    num_reordering_candidates=400)\n",
        "scann3.index_from_dataset(\n",
        "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
        ")\n",
        "\n",
        "_, titles3 = scann3(test_flat, k=10)\n",
        "print(f\"Recall: {compute_recall(titles_ground_truth, titles3):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Isgpm7b9rgE"
      },
      "source": [
        "`scann3` delivers about a 3% absolute recall gain over `scann` while also delivering lower latency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiDEWwtr9sKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f809659a-0c2f-429f-af82-977e89b92613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 19.4 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit -n 1000 scann3(np.array([\"42\"]), k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwWKyQgt9uh1"
      },
      "source": [
        "These knobs can be further adjusted to optimize for different points along the accuracy-performance pareto frontier. ScaNN's algorithms can achieve state-of-the-art performance over a wide range of recall targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvlCsKyFU40k"
      },
      "source": [
        "## Further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ikGqmNa9yRG"
      },
      "source": [
        "ScaNN uses advanced vector quantization techniques and highly optimized implementation to achieve its results. The field of vector quantization has a rich history with a variety of approaches. ScaNN's current quantization technique is detailed in [this paper](https://arxiv.org/abs/1908.10396), published at ICML 2020. The paper was also released along with [this blog article](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) which gives a high level overview of our technique.\n",
        "\n",
        "Many related quantization techniques are mentioned in the references of our ICML 2020 paper, and other ScaNN-related research is listed at http://sanjivk.com/."
      ]
    }
  ]
}